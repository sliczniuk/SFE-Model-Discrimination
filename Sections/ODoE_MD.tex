\documentclass[../Article_Design_of_Experiment.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}
\begin{document}
	
	Multiple alternative models are often proposed to describe the same physical situation. To discriminate between these models, a set of new experiments needs to be performed. Each model attempts to predict the model response $y$ as a function of the experimental conditions $\Xi$ and the model parameters $\theta$ (here, $\theta$ refers to the model parameters that describe the extraction kinetics and is a subset of the parameter space $\Theta$). What varies from model to model is the mathematical form of the function and the set of parameters involved, although some of the parameters appearing in different models may possess the same physical interpretation.
	
	Following the work of \citet{Box1967}, \citet{Himmelblau1970} and \citet{Bard1974} the ratio of two probability distributions can be used to indicate the measure of evidence in favor of one model over another. Consequently, $\ln\left( \frac{p_1(y)}{p_2(y)} \right)$ becomes a measure of the odds in favor of choosing hypothesis $H_1$ (i.e., that $p_1(y)$ is the true model) over hypothesis $H_2$ (i.e., that $p_2(y)$ is the true model). Alternatively, the ratio can be interpreted as the information in favor of hypothesis $H_1$ as opposed to hypothesis $H_2$. The so-called "weight of evidence" or expected information in favor of choosing $H_1$ over $H_2$ can be defined through the Kullback–Leibler divergence and is represented by:
	
	{\footnotesize \begin{equation} D_{\mathrm{KL}}\left( p_1(y) | p_2(y) \right) = \int_{-\infty}^{\infty} p_1(y) \ln\left( \frac{p_1(y)}{p_2(y)} \right) dy \end{equation} }
	
	In essence, $D_{\mathrm{KL}}(p_1(y)|p_2(y))$ measures how sharply data drawn from $p_1$ would differ from data drawn from $p_2$. If $D_{\mathrm{KL}}(p_1(y)|p_2(y))$ is large, then it is relatively unlikely that observations coming from $p_1$ would be mistaken for those coming from $p_2$. Conversely, $D_{\mathrm{KL}}(p_2(y)|p_1(y))$ quantifies how "hard" it would be to confuse data from $p_2$ with those fro, $p_1$. One can view these divergences as asymmetric measures of the distance between two probability distributions. If model 1 is correct, then it is desirable to conduct an experiment $\Xi$ that is likely to confirm this, i.e., is expected to produce a larger value of $D_{\mathrm{KL}}\left( p_1(y) | p_2(y) \right)$. Conversely, if model 2 is correct, then the experiment $\Xi$ results in a larger value of $D_{\mathrm{KL}}\left( p_2(y) | p_1(y) \right)$. Both models assume normal error distributions with predicted covariance matrices $\Sigma_1$ and $\Sigma_2$, respectively:
	
	{\footnotesize \begin{equation} p_i(y) = \frac{1}{\sqrt{(2\pi)^{n_Y} |\Sigma_i|}} \exp\left( -\frac{1}{2} (Y - y_i)^\top \Sigma_i^{-1} (Y - y_i) \right) \end{equation} }
	
	where $Y$ is the observed data, $y_i$ is the predicted response from model $i$, and $n_Y$ is the number of measurements.
	
	In advance of performing the experiment $\Xi$, the output $Y$ is unknown, so $D_{\mathrm{KL}}$ cannot be computed directly. However, its expected value under the assumption that model 1 is correct can be calculated:
	
	{\footnotesize \begin{flalign} 
			&D_{\mathrm{KL}}\left( p_1(y) | p_2(y) \right) = \int_{-\infty}^{\infty} p_1(y) \ln\left( \frac{p_1(y)}{p_2(y)} \right) dy = \left\langle \ln\left( \frac{p_1(y)}{p_2(y)} \right) \right\rangle && \nonumber \\ 
			&= \frac{1}{2} \left\langle \ln\left( \frac{|\Sigma_2|}{|\Sigma_1|} \right) - (Y - y_1)^\top \Sigma_1^{-1} (Y - y_1) + (Y - y_2)^\top \Sigma_2^{-1} (Y - y_2) \right\rangle &&
		\end{flalign} }
	
	Using the properties of the multivariate normal distribution, we compute the expected values of the quadratic forms. The Kullback–Leibler divergence becomes:
	
	{\footnotesize \begin{equation} D_{\mathrm{KL}}\left( p_1(y) | p_2(y) \right) = \frac{1}{2} \left[ \ln\left( \frac{|\Sigma_2|}{|\Sigma_1|} \right) - n_Y + \mathrm{tr}\left( \Sigma_2^{-1} \Sigma_1 \right) + (y_2 - y_1)^\top \Sigma_2^{-1} (y_2 - y_1) \right] \end{equation} }
	
	Physically, the term $\ln\left( \frac{|\Sigma_2|}{|\Sigma_1|} \right)$ compares how "wide" the two models’ predicted variances are overall. The $-n_Y$ arises from normalizing constants and can be seen as a base difference between two $n_Y$‐dimensional Gaussians. The trace term $\mathrm{tr}\left( \Sigma_2^{-1} \Sigma_1 \right)$ measures the mismatch between the shapes of the covariance ellipsoids (how elongated or compressed one distribution is relative to the other). The Mahalanobis distance $(y_2 - y_1)^\top \Sigma_2^{-1} (y_2 - y_1)$ measures how far apart the two mean predictions are, scaled by the covariance $\Sigma_2$.
	
	Similarly, by interchanging the roles of 1 and 2, the analogous expression can be computed $D_{\mathrm{KL}}\left( p_2(y) | p_1(y) \right)$ by swapping the indices. Since it is unknown which of the models is correct, both quantities are summed to represent the objective function $j$:
	
	{\footnotesize 
		\begin{equation} 
			j = D_{\mathrm{KL}}\left( p_1(y) | p_2(y) \right) + D_{\mathrm{KL}}\left( p_2(y) | p_1(y) \right) = \int_{-\infty}^{\infty} \left[ p_1(y) - p_2(y) \right] \ln\left( \frac{p_1(y)}{p_2(y)} \right) dy 
	\end{equation} }
	
	The experiment to be designed is the one that maximizes $j$. A large value of $j$ can be obtained if $p_2(y)$ is much than $p_1(y)$, or vice versa. In either case, the outcome shows a strong preference for one model as opposed to the other. The final form of the objective function $j$ is given by:
	
	{\footnotesize \begin{equation} j = -n_Y + \frac{1}{2} \left[ \mathrm{tr}\left( \Sigma_1^{-1} \Sigma_2 + \Sigma_2^{-1} \Sigma_1 \right) + (y_2 - y_1)^\top \left( \Sigma_1^{-1} + \Sigma_2^{-1} \right) (y_2 - y_1) \right] \end{equation} }
	
	$\Sigma_1$ and $\Sigma_2$ as expressing the expected uncertainties or noise structures inherent in each model. The trace terms reflect how dissimilar the predicted noise covariances are, while the quadratic form indicates how far apart the two predicted mean responses lie in light of those covariances. Since one aims to design an experiment so that data sampled from the correct model would strongly disfavour the incorrect one, maximizing jj ensures that, irrespective of which model is actually correct, the measured data will most likely provide a clear discrimination between the two physical hypotheses.
	
	\subsection{Prediction of the Covariance Matrix}
	
	When using a particular model to predict an extraction yield, three distinct sources of inaccuracy typically arise: uncertainties in the estimated parameters, uncertainties in setting the experimental conditions, and measurement noise. Even if the functional form of the model itself is correct, these errors collectively produce deviations between the actual (observed) and the predicted extraction yields. In practice, there is often a small residual bias in the model’s mean predictions; however, if this bias is negligible relative to the overall variability, it can be ignored in favor of a purely variance‐based analysis. Under the additional assumption that these three error sources are statistically independent, the corresponding variances and covariances can be superimposed to give an approximate covariance matrix for the total prediction error.
	
	To frame this more concretely, consider the model output $y=G(\Xi, \theta)$ as the predicted extraction yield, where $\Xi$ represents the operating conditions (such as temperature, pressure, or solvent flow rates) and $\theta$ denotes the estimated model parameters. Let $\delta \theta$, $\delta \Xi$, and $\delta y$ be the random errors associated with each of these three sources. In that case, the observed yield $y_o$ can be written as
	
	{\footnotesize \begin{equation} y_o = G(\Xi + \delta \Xi, \theta + \delta \theta) + \delta y \end{equation} }
	
	Here, $\delta \Xi$ captures potential deviations in how precisely the experimental conditions are implemented (for instance, if the actual temperature differs slightly from the nominal set point), $\delta \theta$ accounts for inaccuracies or uncertainties in the estimated model parameters, and $\delta y$ represents measurement noise associated with the apparatus or measurement sensor.
	
	A Taylor series expansion up to linear terms with respect to $\theta$ and $\Xi$ results with:
	
	{\footnotesize \begin{equation} y_o - y_p = \frac{\partial G}{\partial \Xi} \delta \Xi + \frac{\partial G}{\partial \theta} \delta \theta + \delta y \end{equation} }
	
	where $y_p=G(\Xi, \theta)$ is the model’s prediction at the nominal operating conditions and estimated parameters. Physically, each partial derivative represents a sensitivity measure: it shows how the yield changes locally if the experimental conditions or the model parameters are perturbed. The product of this sensitivity with the corresponding random error, $\delta \theta$ or $\delta \Xi$, reflects how much each type of uncertainty is expected to shift the yield away from its nominal prediction. 
	
	{\footnotesize \begin{equation} \Sigma = \left\langle (y_o - y_p)(y_o - y_p)^\top \right\rangle = \frac{\partial G}{\partial \Xi} \Sigma_\Xi \left( \frac{\partial G}{\partial \Xi} \right)^\top + \frac{\partial G}{\partial \theta} \Sigma_\theta \left( \frac{\partial G}{\partial \theta} \right)^\top + \Sigma_y \end{equation} }
	
	The first term describes the additional uncertainty introduced if the experimental conditions themselves cannot be fixed exactly; it may be dropped if conditions (for example, temperature or flow rate) are perfectly controlled or have negligible variation. The second term quantifies how parameter uncertainty, arising from imperfect fitting of kinetic or equilibrium constants, propagates into the final yield predictions. The third term, reflects random noise in measuring or detecting the yield. This might stem from instrument imprecision or environmental factors, and can be estimated either by looking at residuals from previous calibration measurements or by using specifications provided by the instrument manufacturer.
	
	If the parameters $\theta$ are being estimated in a statistically efficient way (for instance, by a maximum‐likelihood method with large‐sample properties), then $\Sigma \theta$ often approaches the inverse of the Fisher information, thereby saturating the Cramér–Rao lower bound asymptotically.
		
	\subsection{Problem formulation}
	The optimization problem is solved for multiple cases of constant pressure: 100, 125, 150, 175 and 200 bar.
	
	{\footnotesize
		\begin{equation}
			\begin{aligned} 
				&\Xi^* &= \arg &\min_{ T^{in}, F \in \Xi} -j  \\
				&\text{subject to}
				& \dot{x} &= G(x,t,\Theta;\Xi) \\
				&& t_0&=0\quad~\text{min} \\
				&& t_f&=600~\text{min} \\
				&& T^{0} &= T^{in}(t=0) \\
				&& P(t) & \in \{100, 125, 150, 175, 200\}~\text{bar} \\
				%&& \dot{y} = g(x(t)) \\
				&& 30^\circ C \leq &T^{in}(t) \leq 40^\circ C \\
				&& 3.33 \cdot 10^{-5} \leq &F(t) \leq 6.67 \cdot 10^{-5}
			\end{aligned} \label{EQ:Formulation}
	\end{equation} }
	
	Although both models were fitted using the same dataset, they incorporate structurally different extraction kinetics, leading to distinct outputs, especially in regions not covered by the data. New experimental conditions ($\Xi^*$) are determined by identifying the set of controls that causes the greatest divergence between the models’ outputs. The next step is to conduct the experiment and compare the results with the model predictions. The model that most accurately predicts the experimental outcome is retained for further use. 
	
\end{document}













































